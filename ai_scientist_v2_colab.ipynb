{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Scientist-v2 Colab Notebook (Gemini-only)\n",
        "\n",
        "This notebook mirrors the structure of `shinka_tutorial.ipynb` and runs the full pipeline:\n",
        "**Ideation \u2192 BFTS Experiments \u2192 Debugging \u2192 Writeup \u2192 Review**.\n",
        "\n",
        "Notes:\n",
        "- The repo defaults are patched for `gemini-3-pro-preview`.\n",
        "- This notebook stores outputs in Google Drive.\n",
        "- Running LLM-generated code can be unsafe; use a sandboxed environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Colab + Drive setup\n",
        "Mount Drive and choose where to store the repo and outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
        "WORK_ROOT = DRIVE_ROOT / 'ai-scientist-v2'\n",
        "REPO_URL = 'https://github.com/immanuelk1m/AI-Scientist-v2.git'\n",
        "REPO_DIR = WORK_ROOT / 'AI-Scientist-v2'\n",
        "\n",
        "WORK_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "print('Drive root:', DRIVE_ROOT)\n",
        "print('Work root:', WORK_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone repo (or update)\n",
        "Clones into Drive so `experiments/` are persisted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
        "else:\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
        "\n",
        "print('Repo:', REPO_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install Python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "subprocess.run([\"pip\", \"install\", \"-r\", str(REPO_DIR / \"requirements.txt\")], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2b. (Optional) Install LaTeX for PDF writeup\n",
        "Required for `pdflatex` if you want PDF outputs. This can take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Uncomment if you need LaTeX in Colab\n",
        "# !apt-get update -y\n",
        "# !apt-get install -y texlive-latex-extra texlive-fonts-recommended texlive-fonts-extra texlive-science latexmk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. API keys (Gemini)\n",
        "Set `GEMINI_API_KEY`. Optional: `S2_API_KEY` for Semantic Scholar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = getpass(\"GEMINI_API_KEY: \")\n",
        "\n",
        "# Optional (press Enter to skip)\n",
        "_s2 = getpass(\"S2_API_KEY (optional): \")\n",
        "if _s2:\n",
        "    os.environ[\"S2_API_KEY\"] = _s2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Write your ideation topic\n",
        "Edit the fields below and save the markdown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "TOPIC_NAME = \"my_research_topic\"\n",
        "TOPIC_MD = dedent('''\n",
        "# Title\n",
        "Your project title here\n",
        "\n",
        "# Keywords\n",
        "- keyword1\n",
        "- keyword2\n",
        "- keyword3\n",
        "\n",
        "# TL;DR\n",
        "One-sentence summary of the research idea.\n",
        "\n",
        "# Abstract\n",
        "A short abstract describing the task, approach, and expected insights.\n",
        "\n",
        "# Short Hypothesis\n",
        "A concise hypothesis you want to test.\n",
        "\n",
        "# Experiments\n",
        "- Experiment 1: ...\n",
        "- Experiment 2: ...\n",
        "\n",
        "# Risk Factors and Limitations\n",
        "- Limitation 1: ...\n",
        "- Limitation 2: ...\n",
        "''').strip() + \"\\n\"\n",
        "\n",
        "idea_md_path = REPO_DIR / \"ai_scientist\" / \"ideas\" / f\"{TOPIC_NAME}.md\"\n",
        "idea_md_path.write_text(TOPIC_MD)\n",
        "print('Wrote:', idea_md_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ideation (Gemini-only)\n",
        "Generates `<topic>.json` next to your topic file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "MODEL = \"gemini-3-pro-preview\"\n",
        "\n",
        "cmd = [\n",
        "    \"python\",\n",
        "    str(REPO_DIR / \"ai_scientist\" / \"perform_ideation_temp_free.py\"),\n",
        "    \"--workshop-file\",\n",
        "    str(idea_md_path),\n",
        "    \"--model\",\n",
        "    MODEL,\n",
        "    \"--max-num-generations\",\n",
        "    \"20\",\n",
        "    \"--num-reflections\",\n",
        "    \"5\",\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "subprocess.run(cmd, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run BFTS experiments (default params)\n",
        "Uses defaults in `bfts_config.yaml` and Gemini for all stages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "idea_json_path = idea_md_path.with_suffix('.json')\n",
        "\n",
        "cmd = [\n",
        "    \"python\",\n",
        "    str(REPO_DIR / \"launch_scientist_bfts.py\"),\n",
        "    \"--load_ideas\",\n",
        "    str(idea_json_path),\n",
        "    \"--model_writeup\",\n",
        "    MODEL,\n",
        "    \"--model_writeup_small\",\n",
        "    MODEL,\n",
        "    \"--model_citation\",\n",
        "    MODEL,\n",
        "    \"--model_review\",\n",
        "    MODEL,\n",
        "    \"--model_agg_plots\",\n",
        "    MODEL,\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "subprocess.run(cmd, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inspect latest run\n",
        "Lists the newest experiment folder and useful log locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "exp_root = REPO_DIR / \"experiments\"\n",
        "if not exp_root.exists():\n",
        "    raise FileNotFoundError(\"No experiments directory found yet.\")\n",
        "\n",
        "runs = [p for p in exp_root.iterdir() if p.is_dir()]\n",
        "latest_run = max(runs, key=lambda p: p.stat().st_mtime)\n",
        "print('Latest run:', latest_run)\n",
        "\n",
        "print('Log root:', latest_run / 'logs')\n",
        "print('Figures:', latest_run / 'figures')\n",
        "print('PDFs:', list(latest_run.glob('*.pdf')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. View tree plot (HTML)\n",
        "Opens the latest tree plot if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from IPython.display import IFrame, display\n",
        "import glob\n",
        "\n",
        "html_candidates = sorted(glob.glob(str(latest_run / \"logs\" / \"stage_*\" / \"tree_plot.html\")))\n",
        "if html_candidates:\n",
        "    display(IFrame(src=html_candidates[-1], width=\"100%\", height=800))\n",
        "else:\n",
        "    print('No tree_plot.html found yet.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. (Optional) Re-run writeup\n",
        "Useful if writeup failed or you want to regenerate PDFs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Example: re-run writeup\n",
        "# cmd = [\n",
        "#     \"python\",\n",
        "#     str(REPO_DIR / \"ai_scientist\" / \"perform_writeup.py\"),\n",
        "#     \"--folder\",\n",
        "#     str(latest_run),\n",
        "#     \"--model\",\n",
        "#     MODEL,\n",
        "#     \"--big-model\",\n",
        "#     MODEL,\n",
        "# ]\n",
        "# subprocess.run(cmd, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Ensure results are stored in Drive\n",
        "If the repo is in Drive, outputs are already persisted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Repo path:', REPO_DIR)\n",
        "print('Experiments path:', REPO_DIR / 'experiments')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}